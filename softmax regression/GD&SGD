import numpy as np

x = np.loadtxt("./data/iris_x.dat", dtype=float, ndmin=2)
y = np.loadtxt("./data/iris_y.dat", dtype=float, ndmin=2).astype(int)


class Softmax(object):
    def __init__(self, x, y):
        self._x = np.array(x)
        self._y = np.array(y)

        self.y = self.one_hot()

        self.z_score_normalize()

        self.x = np.hstack((np.ones((self.x.shape[0], 1)), self.x))
        self.w = np.random.rand(self.x.shape[1], self.y.shape[1])

    def one_hot(self):
        size = self._y.max() + 1
        y = np.zeros((self._y.shape[0], size))
        y[np.arange(self._y.shape[0]), self._y.flatten()] = 1
        return y

    def z_score_normalize(self):
        x_mean, x_std = np.mean(self._x, axis=0), np.std(self._x, axis=0)
        self.x = (self._x - x_mean) / x_std

    def softmax(self, x):
        s = np.exp(x)
        return s/np.sum(s, axis=1, keepdims=True)

    def max_likelihood_estimation(self, x, y, pred):
        loss = np.sum(-y*np.log2(pred))
        grad = np.matmul(x.T, pred-y)
        return grad, loss

    def cal_acc(self):
        pred = self.softmax(np.matmul(self.x, self.w))
        pred = np.argmax(pred, axis=1)
        acc = np.sum(pred.flatten() == self._y.flatten()) / self.x.shape[0]
        return acc

    def gradient_descent(self, learning_rate=0.05, iter_times=100):
        for i in range(iter_times):
            pred = self.softmax(np.matmul(self.x, self.w))
            grad, loss = self.max_likelihood_estimation(self.x, self.y, pred)
            self.w -= learning_rate*grad

            acc = self.cal_acc()
            print("* iter {}, loss {:.8f}, acc {:.4f}".format(i + 1, loss, acc))

    def stochastic_gradient_descent(self, learning_rate=0.02, iter_times=10):
        ids = np.arange(self.x.shape[0], dtype=int)
        for i in range(iter_times):
            np.random.shuffle(ids)

            for j in ids:
                x, y = self.x[j], self.y[j]
                x = np.reshape(x, (1, -1))
                y = np.reshape(y, (1, -1))
                pred = self.softmax(np.matmul(x, self.w))

                grad, loss = self.max_likelihood_estimation(x, y, pred)
                self.w -= learning_rate*grad

            pred = self.softmax(np.matmul(self.x, self.w))
            grad, loss = self.max_likelihood_estimation(self.x, self.y, pred)
            acc = self.cal_acc()
            print("* iter {}, loss {:.8f}, acc {:.4f}".format(i + 1, loss, acc))


sr = Softmax(x, y)
sr.stochastic_gradient_descent()
# sr.gradient_descent()
